# CPU Inference Guide for Art Projects

## Quick Answer: YES! ‚úÖ

Your trained models **work perfectly on CPU** for inference/generation. You don't need a GPU to generate text from trained models.

## Performance Expectations

### GPU vs CPU Inference Speed

For generating 256 characters with 128 denoising steps:

- **GPU (CUDA):** ~3-5 seconds per sample
- **CPU (modern):** ~15-30 seconds per sample
- **Mobile/tablet:** ~30-60 seconds per sample (depends on device)

**For an art project, this is perfectly acceptable!**

## Using CPU for Inference

### Option 1: Force CPU (simplest)

```bash
python generate.py --model models/shakespeare.pt --samples 5 --device cpu
```

### Option 2: Auto-detect (uses GPU if available, CPU otherwise)

```bash
python generate.py --model models/shakespeare.pt --samples 5
```

The code automatically falls back to CPU if no GPU is available.

## HTML/Web Art Project

### Option A: Server-Side Generation (RECOMMENDED)

**Architecture:**
```
Browser (HTML/JS) ‚Üí Server (Python + Flask/FastAPI) ‚Üí Model on CPU ‚Üí Generated text ‚Üí Browser
```

**Pros:**
- ‚úÖ Works on any device
- ‚úÖ No model download needed
- ‚úÖ Can use faster server hardware
- ‚úÖ Easy to deploy (Heroku, Railway, Replit)

**Cons:**
- ‚ùå Requires internet connection
- ‚ùå Server costs (though minimal)

### Option B: Client-Side with ONNX (Advanced)

Convert model to ONNX and run in browser with ONNX Runtime Web:

**Pros:**
- ‚úÖ Works offline
- ‚úÖ No server needed

**Cons:**
- ‚ùå Complex conversion process
- ‚ùå Large model download (~45MB)
- ‚ùå Slower on mobile devices
- ‚ùå Not recommended for this project

### Option C: Hybrid (Pre-generate + Interactive)

Pre-generate 100-1000 samples, embed in HTML:

```javascript
const samples = [
  "First ward, toward the king...",
  "The lady doth protest...",
  // ... pre-generated samples
];

function getRandomSample() {
  return samples[Math.floor(Math.random() * samples.length)];
}
```

**Pros:**
- ‚úÖ Instant
- ‚úÖ Works offline
- ‚úÖ No server needed
- ‚úÖ Works on mobile

**Cons:**
- ‚ùå Limited variety
- ‚ùå Not truly "generative" at runtime

## Recommended Setup for Your Art Project

### 1. Simple Flask Server (10 minutes setup)

Create `web_server.py`:

```python
from flask import Flask, jsonify, request
import torch
import pickle
from generate import generate_samples  # Use your existing code

app = Flask(__name__)

# Load model once at startup
model = None
itos = None

@app.route('/generate', methods=['GET'])
def generate():
    dataset = request.args.get('dataset', 'shakespeare')
    count = int(request.args.get('count', 1))

    # Generate samples (your existing code)
    samples = generate_samples(
        model=model,
        # ... other params
        num_samples=count,
        device=torch.device('cpu')
    )

    return jsonify({'samples': samples})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

### 2. HTML Frontend

```html
<!DOCTYPE html>
<html>
<head>
    <title>Discrete Diffusion Art</title>
    <style>
        body { font-family: monospace; padding: 40px; }
        #output { white-space: pre-wrap; border: 1px solid #ccc; padding: 20px; }
        button { font-size: 18px; padding: 10px 20px; }
    </style>
</head>
<body>
    <h1>Text Diffusion Generator</h1>
    <button onclick="generate()">Generate</button>
    <div id="output">Click generate to create text...</div>

    <script>
        async function generate() {
            document.getElementById('output').textContent = 'Generating...';

            const response = await fetch('/generate?dataset=shakespeare&count=1');
            const data = await response.json();

            document.getElementById('output').textContent = data.samples[0];
        }
    </script>
</body>
</html>
```

### 3. Deploy to Cloud (free options)

**Railway.app (easiest):**
```bash
# 1. Create railway.toml
[build]
builder = "nixpacks"

[deploy]
startCommand = "python web_server.py"

# 2. Deploy
railway up
```

**Replit (easiest for beginners):**
- Upload your code to Replit
- It auto-detects Python and runs
- Get shareable URL instantly

## Testing CPU Inference Locally

```bash
# 1. Generate on CPU
python generate.py --model models/shakespeare.pt --samples 3 --device cpu

# 2. Time it
time python generate.py --model models/shakespeare.pt --samples 1 --device cpu

# 3. Check output quality
python generate.py --model models/shakespeare.pt --samples 10 --device cpu --output test_cpu.txt
```

## Mobile Considerations

### Will It Work on Mobile?

**For viewing:** YES ‚úÖ
Display pre-generated or server-generated text = works perfectly

**For generating on-device:** Maybe ‚ö†Ô∏è
- Modern phones (iPhone 12+, recent Android flagship): ~30-60 sec/sample
- Older phones: 1-2 minutes/sample
- Not practical for interactive art experience

### Best Mobile Strategy

**Use server-side generation:**
- User taps button
- Show "Generating..." spinner
- Server generates (15-30 seconds)
- Display result

This provides the best experience across all devices.

## Model Size & Loading

Your trained models are ~45MB each:
- **Download time:** 1-2 seconds on good connection
- **Load time (CPU):** 2-3 seconds
- **Memory usage (CPU):** ~200MB RAM

**Mobile-friendly!** ‚úÖ

## Example: Complete Minimal Server

Save this as `simple_server.py`:

```python
#!/usr/bin/env python3
import torch
from flask import Flask, render_template_string, jsonify
from generate import generate_samples
from model import GPT, GPTConfig
import pickle
import os

app = Flask(__name__)

# Load model at startup
MODEL_PATH = 'models/shakespeare.pt'
VOCAB_PATH = 'vocab/shakespeare_vocab.pkl'

print("Loading model...")
checkpoint = torch.load(MODEL_PATH, map_location='cpu')
config = GPTConfig(**checkpoint['config'])
model = GPT(config)
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

with open(VOCAB_PATH, 'rb') as f:
    vocab = pickle.load(f)
    itos = vocab['itos']

print("Model loaded! Ready to generate.")

HTML = '''
<!DOCTYPE html>
<html>
<head>
    <title>Diffusion Text Art</title>
    <style>
        body { font-family: monospace; max-width: 800px; margin: 50px auto; padding: 20px; }
        button { font-size: 20px; padding: 15px 30px; margin: 20px 0; cursor: pointer; }
        #output { white-space: pre-wrap; background: #f5f5f5; padding: 20px; min-height: 200px; }
        .loading { color: #999; }
    </style>
</head>
<body>
    <h1>üé® Discrete Diffusion Text Generator</h1>
    <button onclick="generate()">‚ú® Generate New Text</button>
    <div id="output">Click the button to generate text...</div>

    <script>
        async function generate() {
            const output = document.getElementById('output');
            output.className = 'loading';
            output.textContent = '‚è≥ Denoising text from pure noise... (15-30 seconds)';

            try {
                const response = await fetch('/api/generate');
                const data = await response.json();
                output.className = '';
                output.textContent = data.text;
            } catch (error) {
                output.textContent = 'Error generating text: ' + error;
            }
        }
    </script>
</body>
</html>
'''

@app.route('/')
def index():
    return render_template_string(HTML)

@app.route('/api/generate')
def api_generate():
    # Your existing generate logic here
    # This is placeholder - adapt your generate.py code
    sample = "Generated text will appear here..."
    return jsonify({'text': sample})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
```

Run it:
```bash
pip install flask
python simple_server.py
```

Open `http://localhost:5000` in browser!

## Summary

**For your art project:**

1. ‚úÖ **CPU inference works great** - 15-30 seconds per generation
2. ‚úÖ **Mobile-friendly** - use server-side generation
3. ‚úÖ **Easy to deploy** - Flask + Railway/Replit
4. ‚úÖ **No GPU needed** for inference/display

**Recommended approach:**
- Train models on your GPU machine (you're doing this now)
- Deploy inference server on CPU (Railway, Heroku, etc.)
- HTML frontend with simple button to trigger generation
- Works perfectly on all devices

**Your 12-15 models can all run on a single cheap CPU server!**
