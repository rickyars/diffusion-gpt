# Discrete Diffusion GPT Training Configuration

# Device configuration
device: auto  # 'auto', 'cuda', 'cpu' - auto will use CUDA if available

# Random seed for reproducibility
seed: 42

# Training hyperparameters
training:
  epochs: 25  # 25-50 epochs is usually sufficient (100 is overkill)
  batch_size: 64  # increase to 128 or 256 if GPU memory allows
  learning_rate: 0.0001
  val_split: 0.1  # fraction of data for validation
  eval_interval: 5  # evaluate every N epochs
  save_interval: 5  # save checkpoint every N epochs
  log_interval: 100  # log loss every N batches (was 10, reduced I/O overhead)
  use_compile: false  # torch.compile() for faster training (requires Triton, may not work on Windows)

# Model architecture
model:
  n_layer: 6  # number of transformer layers
  n_head: 6  # number of attention heads
  n_embd: 384  # embedding dimension
  cond_dim: 64  # conditioning dimension for noise
  dropout: 0.2  # dropout probability
  bias: false  # use bias in linear layers
  context_length: 256  # maximum sequence length

# Noise schedule
noise:
  sigma_min: 0.0001
  sigma_max: 20.0

# Sampling/Generation parameters
sampling:
  steps: 128  # number of denoising steps
  temperature: 1.0  # sampling temperature
  num_samples: 10  # number of samples to generate

# Datasets configuration
# Add your datasets here following the template below
# Each dataset should be a .txt file with one document per line

datasets:
  # Example 1: Shakespeare (for testing)
  shakespeare:
    path: datasets/shakespeare.txt
    enabled: true
    description: "Shakespeare's complete works"

  # Example 2: GitHub commits
  github_commits:
    path: datasets/github_commits.txt
    enabled: false
    description: "Terse technical commit messages"

  # Example 3: Amazon reviews
  amazon_reviews:
    path: datasets/amazon_reviews.txt
    enabled: false
    description: "Customer product reviews"

  # Example 4: Goodreads reviews
  goodreads_reviews:
    path: datasets/goodreads_reviews.txt
    enabled: false
    description: "Book reviews from Goodreads"

  # Example 5: Yelp reviews
  yelp_reviews:
    path: datasets/yelp_reviews.txt
    enabled: false
    description: "Restaurant and business reviews"

  # Example 6: Hacker News posts
  hacker_news:
    path: datasets/hacker_news_posts.txt
    enabled: false
    description: "Tech discussion posts"

  # Example 7: Reddit comments
  reddit_comments:
    path: datasets/reddit_comments.txt
    enabled: false
    description: "Casual social media comments"

  # Example 8: YouTube comments
  youtube_comments:
    path: datasets/youtube_comments.txt
    enabled: false
    description: "Video comments"

  # Example 9: arXiv abstracts
  arxiv_abstracts:
    path: datasets/arxiv_abstracts.txt
    enabled: false
    description: "Formal academic paper abstracts"

  # Example 10: Stack Overflow questions
  stackoverflow_questions:
    path: datasets/stackoverflow_questions.txt
    enabled: false
    description: "Technical programming questions"

  # Example 11: Stack Overflow answers
  stackoverflow_answers:
    path: datasets/stackoverflow_answers.txt
    enabled: false
    description: "Technical programming answers"

  # Example 12: Twitter/X posts
  twitter_posts:
    path: datasets/twitter_posts.txt
    enabled: false
    description: "Short social media posts"

  # Example 13: News articles
  news_articles:
    path: datasets/news_articles.txt
    enabled: false
    description: "Journalistic news text"

  # Example 14: Blog posts
  blog_posts:
    path: datasets/blog_posts.txt
    enabled: false
    description: "Personal blog writing"

  # Example 15: Product descriptions
  product_descriptions:
    path: datasets/product_descriptions.txt
    enabled: false
    description: "E-commerce product descriptions"

# Output directories
paths:
  datasets_dir: datasets
  models_dir: models
  outputs_dir: outputs
  vocab_dir: vocab  # store vocabulary files here
