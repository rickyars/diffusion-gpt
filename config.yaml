# Discrete Diffusion GPT Training Configuration

# Device configuration
device: auto  # 'auto', 'cuda', 'cpu' - auto will use CUDA if available

# Random seed for reproducibility
seed: 42

# Training hyperparameters
training:
  epochs: 25  # 25-50 epochs is usually sufficient (100 is overkill)
  batch_size: 64  # increase to 128 or 256 if GPU memory allows
  learning_rate: 0.0001
  val_split: 0.1  # fraction of data for validation
  eval_interval: 5  # evaluate every N epochs
  save_interval: 5  # save checkpoint every N epochs
  log_interval: 100  # log loss every N batches (was 10, reduced I/O overhead)
  use_compile: false  # torch.compile() for faster training (requires Triton, may not work on Windows)
  skip_completed: true  # skip datasets that have completed training (final .pt file exists)

# Model architecture
model:
  n_layer: 6  # number of transformer layers
  n_head: 6  # number of attention heads
  n_embd: 384  # embedding dimension
  cond_dim: 64  # conditioning dimension for noise
  dropout: 0.2  # dropout probability
  bias: false  # use bias in linear layers
  context_length: 256  # maximum sequence length

# Noise schedule
noise:
  sigma_min: 0.0001
  sigma_max: 20.0

# Sampling/Generation parameters
sampling:
  steps: 128  # number of denoising steps
  temperature: 1.0  # sampling temperature
  num_samples: 10  # number of samples to generate

# Datasets configuration
# Add your datasets here following the template below
# Each dataset should be a .txt file with one document per line

datasets:
  # Example 1: Shakespeare (for testing)
  shakespeare:
    path: datasets/shakespeare.txt
    enabled: true
    description: "Shakespeare's complete works"

  # Example 2: Reddit
  github_commits:
    path: datasets/reddit.txt
    enabled: true
    description: "Anonymized comments / scores from 40 subreddits, in uniform number (25000 each)"

  # Example 3: Stack Overflow
  amazon_reviews:
    path: datasets/stack_overflow.txt
    enabled: true
    description: "Text from 10% of Stack Overflow questions and answers on programming topics"

  # Example 4: Amazon
  amazon_reviews:
    path: datasets/amazon.txt
    enabled: true
    description: "34,686,770 Amazon reviews from 6,643,669 users on 2,441,053 products"

  # Example 5: Yelp
  amazon_reviews:
    path: datasets/yelp.txt
    enabled: true
    description: "598,000 review from yelp"

# Output directories
paths:
  datasets_dir: datasets
  models_dir: models
  outputs_dir: outputs
  vocab_dir: vocab  # store vocabulary files here
